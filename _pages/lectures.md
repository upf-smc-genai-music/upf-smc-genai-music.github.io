---
layout: schedule
permalink: /lectures/
title: Lectures
---

* (The list will be replaced with the table of contents.)
{:toc}

***

## Part 1

### Lecture 1: Course Introduction

**When**: January 12th 2026, 14:00–15:00 Room 52.329

**What**: Introduction to the course, its structure, philosophy, and evaluation.

Before taking this lecture, students are expected to have watched the following videos from The Sound of AI’s *Generative Music AI Course*:

1. [What's Generative Music?](https://www.youtube.com/watch?v=9QNG56fc_l8&list=PL-wATfeyAMNqAPjwGT3ikEz3gMo23pl-D&index=2)
2. [History of Generative Music](https://www.youtube.com/watch?v=3znKoIUrgDI&list=PL-wATfeyAMNqAPjwGT3ikEz3gMo23pl-D&index=3)
3. [Use Cases](https://www.youtube.com/watch?v=Fg3TGfbEL64&list=PL-wATfeyAMNqAPjwGT3ikEz3gMo23pl-D&index=4)
4. [Ethical Implications](https://www.youtube.com/watch?v=DCaE5776Rqg&list=PL-wATfeyAMNqAPjwGT3ikEz3gMo23pl-D&index=5)
5. [Symbolic Vs Audio Generation](https://www.youtube.com/watch?v=VYxcHHJNTR0&list=PL-wATfeyAMNqAPjwGT3ikEz3gMo23pl-D&index=6)
6. [Generative Techniques](https://www.youtube.com/watch?v=W-_eWSrQ_vU&list=PL-wATfeyAMNqAPjwGT3ikEz3gMo23pl-D&index=7)
7. [Limitations and Future Vision](https://www.youtube.com/watch?v=viR9q61wV4Q&list=PL-wATfeyAMNqAPjwGT3ikEz3gMo23pl-D&index=8)

### Lecture 2: End-to-End Generative Music Project
**When**: January 12th 2026, 15:00–16:30 Room 52.329

**What**:
* Steps to run a generative music project in a real-world setting
* Types of symbolic music data
* Real-world challenges, tips and tricks

### Lecture 3: Evaluation
**When**: January 13th 2026, 10:30–11:30 Room 52.101

**What**:
* Objective evaluation metrics
* Subjective evaluation metrics
* Expert-based evaluation metrics
* Market-driven evaluation metrics
* Real-world problems, and possible solutions

### Lecture 4: Genetic Algorithms
**When**: January 13th 2026, 11:30–13:00 Room 52.101

**What**:
* Genetic algorithms for music generation
* Real-world experience / challenges implementing this technique
* GenJam
* Exercises and practical challenges

Before taking this lecture, students are expected to have watched the following videos and coded along the code walkthrough from *The Sound of AI’s Generative Music AI Course*:
1. Genetic Algorithms [[video](https://www.youtube.com/watch?v=CAVy7OZ87mE&list=PL-wATfeyAMNqAPjwGT3ikEz3gMo23pl-D&index=15)] [[slides](https://github.com/musikalkemist/generativemusicaicourse/blob/main/15.%20Genetic%20algorithms/Slides/15.%20Genetic%20algorithms.pdf)]
2. Melody Harmonization with Genetic Algorithms [[video](https://www.youtube.com/watch?v=AmtLrd-cYSY&list=PL-wATfeyAMNqAPjwGT3ikEz3gMo23pl-D&index=16)] [[code](https://github.com/musikalkemist/generativemusicaicourse/blob/main/16.%20Melody%20harmonization%20with%20genetic%20algorithms/Code/geneticmelodyharmonizer.py)]

### Lecture 5: Transformers

**When**: January 26th 2026, 14:00–16:30 Room 52.329

**What**:

* Transformers for music generation.
* Flipped classroom: Group interview activity
* Real-world experience / challenges implementing this technique


Before taking this lecture, students are expected to have watched the following videos and coded along the code walkthrough from The Sound of AI’s *Generative Music AI Course:*

1. Transformers Explained Easily: Part 1 \[[video](https://www.youtube.com/watch?v=FtXT-AFzSvg&list=PL-wATfeyAMNqAPjwGT3ikEz3gMo23pl-D&index=17)\] \[[slides](https://github.com/musikalkemist/generativemusicaicourse/blob/main/17.%20Transformers%20-%20Part%201/Slides/17.%20Transformers%20-%20Part%201.pdf)\]
2. Transformers Explained Easily: Part 2 \[[video](https://www.youtube.com/watch?v=ctbvMnbylsA&list=PL-wATfeyAMNqAPjwGT3ikEz3gMo23pl-D&index=18)\] \[[slides](https://github.com/musikalkemist/generativemusicaicourse/blob/main/18.%20Transformers%20-%20Part%202/Slides/18.%20Transformers%20Part%202.pdf)\]
3. Melody Generation with Transformers \[[video](https://www.youtube.com/watch?v=j4LABY2d7k4&list=PL-wATfeyAMNqAPjwGT3ikEz3gMo23pl-D&index=19)\] \[[code](https://github.com/musikalkemist/generativemusicaicourse/tree/main/19.%20Melody%20generation%20with%20transformers/Code)\]

### Lecture 6: Transformer Tokenizers
**When**: January 27th 2026, 10:30–11:30 Room 54.004

**What**:
* Tokenizers for MIDI / symbolic representations
* MidiTok


### Lecture 7:  Advanced Transformer Architectures
**When**: January 27th 2026, 11:30–13:00 Room 54.004

**What**:
* SOTA transformer systems for symbolic music generation
* Discuss papers, and debate systems’ outputs
* MuseFormer
* MuPT

Before taking this lecture, students are expected to have read the following papers:
* Museformer: Transformer with Fine- and Coarse-Grained Attention for Music Generation [[paper](https://proceedings.neurips.cc/paper_files/paper/2022/file/092c2d45005ea2db40fc24c470663416-Paper-Conference.pdf)][[website](https://ai-muzic.github.io/museformer/)]
* MuPT: Symbolic Music Generative Pre-trained Transformer [[blog](https://map-mupt.github.io/)][[paper](https://arxiv.org/abs/2404.06393)]

Optional reading:
* Music Transformer (the first transformer for symbolic music generation) [[paper](https://arxiv.org/abs/1809.04281)][[blog + demos](https://magenta.withgoogle.com/music-transformer)]
* Anticipatory Music Transformer: A Controllable Infilling Model for Music [[blog](https://crfm.stanford.edu/2023/06/16/anticipatory-music-transformer.html?idx=14#prompted-example)][[paper](https://arxiv.org/pdf/2306.08620)]


### Lecture 8: Code Assignments

**When**: February 16th 2026, 14:00-16:00 Room 52.329

**What**:

* Present 2x code assignments
* Evaluate results together + get feedback

### Lecture 9: Career Advice
**When**: February 16th 2026, 16:00–16:30 Room 52.329

**What**:
* How to get a career in generative music
* Q&A

### Lecture 10: Final Project Presentations
**When**: February 17th 2026, 10:30–13:00 Room 52.105

**What**:
* Present final project
* Get feedback

### Extra Tutorial Class: Hugging Face Transformers
This class is taught by Fernando and Andreas.

**When**: January 29th 2026, 14:00–16:30 Room 52.329

**What**:
* Inference + Fine Tuning with Hugging Face Transformers
* Using pre-trained symbolic models

Before taking this lecture, students are expected to have installed the following libraries and coded along the code walkthrough:
* Hugging Face Transformers [[blog](https://huggingface.co/docs/transformers/en/quicktour)]
* Hugging Face MidiGPT2 [[blog](https://huggingface.co/xingjianll/midi-gpt2)]
* Hugging Face PEFT (Parameter-Efficient Fine-Tuning) [[blog](https://huggingface.co/docs/peft/quicktour)]
* Hugging Face LoRA [[blog](https://huggingface.co/docs/peft/package_reference/lora)]

---



## Part 2 (Audio)

Slides [[CMC_0_Intro](https://drive.google.com/file/d/17SqQoT6LKyf9Xq2g41B6IZJDZdC51uW3/view?usp=drive_link)]

### Week 1: Audio modeling Introduction; Sound Model Factory

**When:** **Monday, February 9th** 2026, 14:00–17:00 Room 52.329

**What:**

* Introduction to the second part of the course on generative audio.
* Discussion about the main ideas, audio representations, and architectures commonly used.
* Sound Model Factory approach to creating playable audio models.
* Audio representations 

Lecture preparation :

1. Engel, J., Agrawal, K. K., Chen, S., Gulrajani, I., Donahue, C., & Roberts, A. **(2019**). **Gansynth**: Adversarial neural audio synthesis. *arXiv preprint arXiv:1902.08710*.  [[Link]](https://arxiv.org/abs/1902.08710)
2. Wyse, L., Kamath, P., & Gupta, C. (2022, April). Sound model factory: An integrated system architecture for generative audio modelling. In *International Conference on Computational Intelligence in Music, Sound, Art and Design (Part of EvoStar)* (pp. 308-322). Cham: Springer International Publishing. [[Link]](https://arxiv.org/abs/2206.13085)
3. [OK - just a quick browse of this one]
   1. Van Den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., … & Kavukcuoglu, K. (**2016**). **Wavenet**: A generative model for raw audio. *arXiv preprint arXiv:1609.03499*, *12*.



* **Pre-lecture Quiz**: [[Link](https://docs.google.com/forms/d/e/1FAIpQLSeDo6B3ME-T7ciLZMyZW25bjRBqoiFGE588qVN5Qg0tAZ2jRw/viewform?usp=publish-editor)]

  

Slides [[CMC_1_DataDrivenSoundModeling.pptx](https://docs.google.com/presentation/d/1ATPaYaF3D42np1LL3QVHgDGBCc7xTuyV/edit?usp=sharing&ouid=113968304967495650516&rtpof=true&sd=true)] (download to access embedded audios)

Slides [[CMC_2a_Representation&SoundModeling.pptx](https://docs.google.com/presentation/d/1jrdsVHqHxQP2ACe9oFUsbgt3-zXg2a08/edit?usp=sharing&ouid=113968304967495650516&rtpof=true&sd=true)]  (download to access embedded audios)




---
### ---   Week XX : Back to Symbolic --- 

**When:** **February 16th, 17th - Symbolic Final Projects

---



### Week 2: Representation & Codecs

**When:** **Monday, February 23rd** 2026, 14:00–17:00 Room 52.329

**What:**

* From Audio representations to Codecs

 Lecture preparation :

1. Kumar, R., Seetharaman, P., Luebs, A., Kumar, I., & Kumar, K. (2024). High-fidelity audio compression with improved rvqgan. Advances in Neural Information Processing Systems, 36. [[Link](https://arxiv.org/pdf/2306.06546)] - The Descript Audio Codec (DAC) that we will look at more closely next week.

2. Garcia, H. F., Seetharaman, P., Kumar, R., & Pardo, B. (2023). Vampnet: Music generation via masked acoustic token modeling. arXiv preprint arXiv:2307.04686. [[Link](https://arxiv.org/pdf/2307.04686)] - Uses the DAC in fun and interesting ways, helps to understand and motivate tokenization.

Optional:

3. Van Den Oord, A., & Vinyals, O. (2017). Neural discrete representation learning. Advances in neural information processing systems, 30. [[Link](https://arxiv.org/pdf/1711.00937)] - 5000 citations - historically important paper, a good image that Kumar et al should really have, and a section specifically on audio.

Pre-lecture Quiz: [[Link](https://docs.google.com/forms/d/e/1FAIpQLScxSM5kHF7ZEAap9wCjylLc9iMsrM_daWTdYXDnIgHpFaJGaQ/viewform?usp=publish-editor)]

Slides: [available before class time]

### Week 3: DDSP and Rave

**When:** **Monday,  March 2nd** 2026, 14:00–17:00 Room 52.329

**What:**

* DDSP, RAVE, BRAVE

 Lecture preparation :



### Week 4: Transformers for Audio

**When:** **Monday,  March 9th** 2026, 14:00–17:00 Room 52.329

**What:**
* Core transformer architecture, considerations for audio

  

### Week 5: Text2Audio & Evaluation for generative models

**When:** **Monday,  March 16th** 2026, 14:00–17:00 Room 52.329

**What:**

* Overview of Diffusion and Transformer models for text-to-audio - CLAP

* Objective and subjective approaches to evaluating generative audio

  

---



### A few good way-background papers on Generative Audio

- Van Den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., … & Kavukcuoglu, K. (**2016**). **Wavenet**: A generative model for raw audio. *arXiv preprint arXiv:1609.03499*, *12*.

- Kumar, K., Kumar, R., De Boissiere, T., Gestin, L., Teoh, W. Z., Sotelo, J., … & Courville, A. C. (**2019**). **Melgan**: Generative adversarial networks for conditional waveform synthesis. *Advances in neural information processing systems*, *32*. [keywords: Vocoder; Phase construction]

- Engel, J., Agrawal, K. K., Chen, S., Gulrajani, I., Donahue, C., & Roberts, A. **(2019**). **Gansynth**: Adversarial neural audio synthesis. *arXiv preprint arXiv:1902.08710*. [keywords: conditional training]

- Engel, J., Hantrakul, L., Gu, C., & Roberts, A. (**2020**). **DDSP**: Differentiable digital signal processing. *arXiv preprint arXiv:2001.04643*.

   [keywords: inductive bias, signal processing units, real time]

- Caillon, A., & Esling, P. (**2021**). **RAVE**: A variational autoencoder for fast and high-quality neural audio synthesis. *arXiv preprint arXiv:2111.05011*. [keywords: conditional training]

- Huzaifah, M., & Wyse, L. (2021). Deep generative models for musical audio synthesis. *Handbook of artificial intelligence for music: foundations, advanced approaches, and developments for creativity*, 639-678. [keywords: “review” paper]

- Wyse, L., Kamath, P., & Gupta, C. (2022, April). Sound model factory: An integrated system architecture for generative audio modelling. In *International Conference on Computational Intelligence in Music, Sound, Art and Design (Part of EvoStar)* (pp. 308-322). Cham: Springer International Publishing. [keywords: playability; latent space]

- Garcia, H. F., Seetharaman, P., Kumar, R., & Pardo, B. (2023). Vampnet: Music generation via masked acoustic token modeling. *arXiv preprint arXiv:2307.04686*.

   [keywords: transformer, in-painting, masking for training, codecs]

- Evans, Z., Parker, J. D., Carr, C. J., Zukowski, Z., Taylor, J., & Pons, J. (**2024**). **Stable audio open**. *arXiv preprint arXiv:2407.14358*. [keywords: Text-2-audio; Open (data, weights, code, latent diffusion]
